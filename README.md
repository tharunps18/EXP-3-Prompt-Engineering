# EXP-3-PROMPT-ENGINEERING

## Title
Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta

---

## Aim
To systematically evaluate and compare the performance, user experience, and response quality of leading 2024 prompting tools—ChatGPT, Claude, Bard, Cohere Command, and Meta—across two representative use cases:  
1. Summarizing complex textual content.  
2. Answering domain-specific technical questions.  

The goal is to establish a structured framework to assess prompting effectiveness, highlight strengths and weaknesses, and provide insights into the optimal usage scenarios for each platform.

---

## Experiment
A controlled experiment was designed where **identical prompts** were tested across all five AI platforms. The evaluation process focused on two distinct tasks (summarization and technical Q&A) and applied a **consistent scoring methodology** across multiple criteria.

Each response was analyzed in terms of **accuracy, coherence, fluency, creativity, and user experience.**

---

## Algorithm 

### Step 1: Define Scope and Objectives
- Select multiple AI platforms for comparison: ChatGPT, Claude, Bard, Cohere Command, and Meta.
- Narrow down to two critical use cases:  
  1. **Summarization Task:** Condense a long piece of text into a short, accurate, and coherent summary.  
  2. **Technical Q&A Task:** Provide precise and structured answers to domain-specific technical questions.  
- Establish evaluation goals:
  - Identify comparative strengths.
  - Understand trade-offs between accuracy, creativity, and usability.

---

### Step 2: Design Prompts
- Construct a **standardized prompt set** for both tasks.
- Ensure neutrality of phrasing to avoid bias toward any single platform.
- Prompts include:
  - Summarization Example: *“Summarize the following 500-word article into 100 words, preserving all key points.”*
  - Technical Q&A Example: *“Explain the time complexity of binary search and compare it with linear search.”*

---

### Step 3: Execute Prompts Across Platforms
- Run each standardized prompt on all five platforms.
- Collect raw responses **without modification.**
- Record metadata:
  - Response time
  - Formatting style
  - Any errors, hallucinations, or disclaimers

---

### Step 4: Define Evaluation Criteria
1. **Accuracy:** Is the content factually correct?  
2. **Coherence:** Is the response logically structured and easy to follow?  
3. **Conciseness & Completeness:** Does it cover all necessary points without redundancy?  
4. **Creativity & Adaptability:** Does the model handle ambiguous or complex prompts well?  
5. **User Experience:**  
   - Readability of formatting (bullets, tables, clarity)  
   - Response speed and stability  
   - Handling of edge cases  

---

### Step 5: Apply Scoring System
- Create a rubric with **1–5 scores** for each criterion.  
- Example:
  - 1 = Poor  
  - 2 = Fair  
  - 3 = Average  
  - 4 = Good  
  - 5 = Excellent  
- Maintain a comparative scoring matrix for each platform across both tasks.

---

### Step 6: Analyze Results
- Identify performance trends:
  - Which platform is best for factual precision?  
  - Which platform produces the most fluent summaries?  
  - Which platform is strongest in technical depth?  
- Compare strengths and weaknesses task-wise.

---

### Step 7: Conclude Findings
- Summarize insights into platform suitability:
  - Best for students and researchers.  
  - Best for general users.  
  - Best for technical domains.  
- Provide recommendations for practical adoption.

---

## Output 

### 1. Introduction
Prompting has emerged as the core interaction method with Large Language Models (LLMs). In 2024, platforms such as **ChatGPT (OpenAI), Claude (Anthropic), Bard (Google), Cohere Command (Cohere), and Meta AI (LLaMA models)** dominate the AI ecosystem. Evaluating these tools systematically is critical for selecting the right AI assistant for academic, professional, and research purposes.

---

### 2. Methodology
- **Platforms Tested:** ChatGPT, Claude, Bard, Cohere Command, Meta  
- **Tasks Selected:** Summarization, Technical Q&A  
- **Criteria:** Accuracy, Coherence, Conciseness, Creativity, User Experience  
- **Evaluation Style:** Both qualitative (observations) and quantitative (scoring table).

---

### 3. Comparative Analysis

#### Use Case 1: Summarization
- **ChatGPT:** Produces highly fluent, human-like summaries. Strong in clarity and coherence. Occasionally verbose.  
- **Claude:** Very faithful to original text, avoids hallucinations, sometimes overly cautious.  
- **Bard:** Concise, but sometimes misses critical points.  
- **Cohere Command:** Balanced, though summaries lack polish compared to ChatGPT.  
- **Meta:** Strong in speed and brevity but limited in contextual nuance.  

#### Use Case 2: Technical Q&A
- **ChatGPT:** Strong in explaining algorithms with examples. Provides detailed comparisons.  
- **Claude:** Exceptionally cautious; explanations precise but less creative.  
- **Bard:** Direct, factually decent, but sometimes shallow in depth.  
- **Cohere Command:** Provides structured answers but lacks domain richness.  
- **Meta:** Concise responses, but tends to simplify too much.  

---

### 4. Results Table (Sample Scoring Matrix)

| Platform        | Accuracy | Coherence | Conciseness | Creativity | UX  | Total (25) |
|-----------------|----------|-----------|-------------|------------|-----|------------|
| ChatGPT         | 5        | 5         | 4           | 5          | 5   | 24         |
| Claude          | 5        | 4         | 4           | 3          | 4   | 20         |
| Bard            | 4        | 4         | 3           | 3          | 4   | 18         |
| Cohere Command  | 4        | 3         | 4           | 3          | 4   | 18         |
| Meta            | 3        | 3         | 4           | 2          | 3   | 15         |

---

### 5. Observations
- **ChatGPT:** Best all-rounder for fluency, depth, and adaptability.  
- **Claude:** Strong in factual accuracy and safe responses, ideal for critical tasks.  
- **Bard:** Quick and concise, suitable for casual queries.  
- **Cohere Command:** Reliable but less advanced in creativity.  
- **Meta:** Fast and lightweight, but limited depth.  

---

### 6. Conclusion
The comparative study demonstrates that **ChatGPT** remains the strongest tool overall for diverse prompting needs in 2024, excelling in both summarization and technical reasoning. **Claude** is preferred where factual reliability and safe content are paramount. **Bard, Cohere Command, and Meta** serve as supplementary tools depending on the context—whether for quick lookups, lightweight deployment, or cost-efficiency.

By applying a **systematic algorithmic evaluation** and **multi-criteria analysis**, this experiment provides a replicable framework for future benchmarking of prompting tools.

---
